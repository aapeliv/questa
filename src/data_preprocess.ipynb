{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010a5fc4",
   "metadata": {},
   "source": [
    "This program  performs data cleaning and summarizing to required dataframes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b052cc7-f5aa-4f8b-83e2-3af90f8252ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2280e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3ecc5",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27064461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and extract abstracts (or titles)\n",
    "\n",
    "papers = pd.read_csv('/data/questa_data.csv')\n",
    "papers = papers[['Title', 'Abstract', 'PubYear']]\n",
    "papers = papers.iloc[:-6 , :]   #drop last 6 rows belonging to PubYear = 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562d16f",
   "metadata": {},
   "source": [
    "## Text Pursing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ffcab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the text to lowercase\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['Abstract'].map(lambda x: str(x).lower())\n",
    "\n",
    "# Remove punctuation (this kept the words such as M/M/s as it is, and seperators with '-' as it is)\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub('[,;:.!?{}()\\[\\]\\']', '', str(x))) \n",
    "\n",
    "#remove '-, &, +,=,v_' and replace with empty space. \n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub('[\\&+_-]', ' ', str(x))) \n",
    "\n",
    "#remove non-alphanumeric characters, but keep empty spaces and '/' symbol\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(r'[^A-Za-z0-9 /]', '', str(x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294945c9",
   "metadata": {},
   "source": [
    "## Re-word some texts and change Kendall's notations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7370dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename 'markovian' as 'markov'\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub('markovian', 'markov', str(x))) \n",
    "\n",
    "#rename 'polling' as 'poll', as lemmatization sometimes assigns 'pollong =poll'\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub('polling', 'poll', str(x))) \n",
    "\n",
    "#rename 'regular variation' as 'regvariation', to concatanate the words\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub('regular variation', 'regvariation', str(x))) \n",
    "\n",
    "#collect words  in the themses of 'models', 'methods'and 'concepts' to join\n",
    "#'single server words'\n",
    "ss_words = '|'.join(['single-server', 'single server']) \n",
    "#'multi server words'\n",
    "ms_words = '|'.join(['multi-server', 'multi server', 'multiple-server', 'multiple server', 'multiple servers'])\n",
    "#'queueing network'\n",
    "qn_words = '|'.join(['queueing network', 'queueing networks', 'network of queues'])\n",
    "#'priority queue'\n",
    "pq_words = '|'.join(['priority queues', 'priority queue', 'priority queueing'])\n",
    "#'laplace transform'\n",
    "lt_words = '|'.join(['laplace transform', 'laplace transforms', 'laplace-stieltjes transforms',\n",
    "                     'laplace-stieltjes transform', 'laplace stieltjes transform', 'laplace stieltjes transforms'])\n",
    "#'large deviation'\n",
    "ld_words = '|'.join(['large deviation', 'large deviations', 'large-deviation', 'large-deviations'])\n",
    "#'fluid limit'\n",
    "fl_words = '|'.join(['fluid limit', 'fluid limits', 'fluid-limit'])\n",
    "#'tail asymptotic'\n",
    "ta_words = '|'.join(['tail asymptotic', 'tail asymptotics'])\n",
    "#'product form'\n",
    "pf_words = '|'.join(['product form', 'product forms', 'product-form'])\n",
    "#'bandwidth'. Lemmatization removed the word 'bandwidth'. so change to 'widthband'\n",
    "bw_words = '|'.join(['bandwidth', 'bandwidths'])  \n",
    "\n",
    "#Define single and multi server queue types \n",
    "single_s_types = '|'.join(['\\w*\\/\\w*\\/1\\/\\w*', '\\w*\\/\\w*\\/1']) #this works \n",
    "multi_s_types = '|'.join(['\\w*\\/\\w*\\/\\w*\\/\\w*', '\\w*\\/\\w*\\/\\w*']) \n",
    "\n",
    "#relabel the words types\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(ss_words, 'singleserver', str(x))) \n",
    "\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(ms_words, 'multiserver', str(x))) \n",
    "\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(qn_words, 'queuenetwork', str(x))) \n",
    "\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(pq_words, 'priorityqueue', str(x))) \n",
    "\n",
    "#this produced both 'laptransform' and 'laplace transform'\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(lt_words, 'laptransform', str(x)))  \n",
    "\n",
    "#this also kept words 'largedeviation' and 'largedeviations(just once)'\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(ld_words, 'largedeviation', str(x))) \n",
    "\n",
    "#this also kept words 'limitfluid' and 'fluid limit'(just once, no an error)'\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(fl_words, 'limitfluid', str(x))) \n",
    "\n",
    "#this also kept words 'tailasymptotic' and 'tail asymptotic(just once, not an error)'\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(ta_words, 'tailasymptotic', str(x))) \n",
    "\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(pf_words, 'productform', str(x))) \n",
    "\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(bw_words, 'widthband', str(x))) \n",
    "\n",
    "\n",
    "\n",
    "#relabel  queue types (Maybe can combine these two)\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(single_s_types, 'singleserver', str(x))) \n",
    "\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: re.sub(multi_s_types, 'multiserver', str(x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62ef53",
   "metadata": {},
   "source": [
    "## Tokenize  and remove stop words \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9625a5",
   "metadata": {},
   "source": [
    "#### Tokenize (convert each sentence into words) and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3822b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new words to stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'queue', 'queues', 'queueing', 'editorial', \n",
    "                    'abstract','nan'])\n",
    "\n",
    "#tokenize, and convert to lowercase and return list of structure\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), min_len = 3, deacc=True))\n",
    "\n",
    "#remove stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "data = papers.paper_text_processed.values.tolist()   \n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)   # a list of lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ed7e5",
   "metadata": {},
   "source": [
    "##   Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54af97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "data_lemmatized = lemmatization(data_words)    # a list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bf91c",
   "metadata": {},
   "source": [
    "## Create a dataframe containing lemmatized abstracts and  topics from 'models', 'methods' 'concepts'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799391bf-ee3c-41c3-9e9a-d7fcd07478d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the list of models, methods, concepts\n",
    "\n",
    "models =  ['singleserver', 'multiserver', 'queuenetwork', 'poll', 'vacations', 'priorityqueue', 'tandem']\n",
    "methods =  ['laptransforms', 'largedeviation', 'diffusion', 'limitfluid']\n",
    "concepts = ['insensitivity', 'tailasymptotic', 'productform', 'reversibility',  'widthband', 'stability']\n",
    "\n",
    "lemmatized_models = lemmatization(list(sent_to_words(models)))   #this words\n",
    "lemmatized_models = [\" \".join(sub_list) for sub_list in lemmatized_models]  #this works\n",
    "\n",
    "lemmatized_methods = lemmatization(list(sent_to_words(methods)))   #this words\n",
    "lemmatized_methods = [\" \".join(sub_list) for sub_list in lemmatized_methods]  #this works\n",
    "\n",
    "lemmatized_concepts = lemmatization(list(sent_to_words(concepts)))   #this words\n",
    "lemmatized_concepts = [\" \".join(sub_list) for sub_list in lemmatized_concepts]  #this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb4ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with papers[PubYear] column \n",
    "\n",
    "pub_years = papers['PubYear']\n",
    "df_topics = pub_years.to_frame()  \n",
    "\n",
    "flaten_data_lemmatized = [\" \".join(sub_list) for sub_list in data_lemmatized]\n",
    "df_topics['lemmatized_Abstract'] = flaten_data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca90b60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1750, 19)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if words in lemmatized_models,methods,concepts  lists are in the data_lemmatized list \n",
    "#(this is a list of pre-processed text)\n",
    "\n",
    "lemmatized_text = [\" \".join(sub_list) for sub_list in data_lemmatized]  #flatten the list of lists\n",
    "#print(lemmatized_text)\n",
    "\n",
    "#create boolian columns in df_topics for every word in lemmatized_models,methods,concepts lists \n",
    "for model in lemmatized_models:\n",
    "    df_topics[model] = [t.find(model) != -1 for t in lemmatized_text]   #models\n",
    "    \n",
    "for method in lemmatized_methods:\n",
    "    df_topics[method] = [t.find(method) != -1 for t in lemmatized_text] #methods\n",
    "\n",
    "for concept in lemmatized_concepts:\n",
    "    df_topics[concept] = [t.find(concept) != -1 for t in lemmatized_text] #methods\n",
    "\n",
    "df_topics.shape   #19 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f87c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_models_yrs = [] #a place holder for PubYear corresponding to each column in df column when the column has a 'True'  value\n",
    "lst_methods_yrs = []\n",
    "lst_concepts_yrs = []\n",
    "\n",
    "#get the PubYears corresponding to each model types in  lemmatized_models\n",
    "for model in lemmatized_models:  \n",
    "    l1 = list(df_topics.PubYear[df_topics[model]])  #\n",
    "    lst_models_yrs.append(l1)\n",
    "\n",
    "#get the PubYears corresponding to each method types in  lemmatized_methods\n",
    "for method in lemmatized_methods:  \n",
    "    l2 = list(df_topics.PubYear[df_topics[method]])  #\n",
    "    lst_methods_yrs.append(l2)\n",
    "\n",
    "#get the PubYears corresponding to each concept types in  lemmatized_concepts\n",
    "for concept in lemmatized_concepts:  \n",
    "    l3 = list(df_topics.PubYear[df_topics[concept]])  #\n",
    "    lst_concepts_yrs.append(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ad7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the df_topics to csv file\n",
    "df_topics.to_csv('/results/abstracts_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf77e0df",
   "metadata": {},
   "source": [
    "## Create dataframe for themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dad819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_lst_models = [item for sublist in lst_models_yrs for item in sublist] #flatenns the lst\n",
    "flat_lst_methods = [item for sublist in lst_methods_yrs for item in sublist] #flatenns the lst\n",
    "flat_lst_concepts = [item for sublist in lst_concepts_yrs for item in sublist] #flatenns the lst\n",
    "\n",
    "def count_theme_yrs(lst):\n",
    "    count = sorted(Counter(lst).items())\n",
    "    x = [i[0] for i in count]\n",
    "    y = [i[1] for i in count]\n",
    "    new_y = [round(p/q, 2) for p,q in zip(y, numpapers)]\n",
    "    return x, new_y\n",
    "\n",
    "x_model, y_model = count_theme_yrs(flat_lst_models)\n",
    "x_method, y_method = count_theme_yrs(flat_lst_methods)\n",
    "x_concept, y_concept = count_theme_yrs(flat_lst_concepts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449eb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_length = [len(y_model),len(y_method), len(y_concept)] #\n",
    "\n",
    "#compare x_model and x_method and find the index of element in x_model which are not in x_method\n",
    "diff_lst = set(x_model) ^ set(x_method)   #get non-common elements\n",
    "noncommon_idx_method = [x_model.index(x) for x in diff_lst] #get index of x_model where noncommon elements apppears\n",
    "#insert 0 at index given by noncommon_idx_method\n",
    "new_y_method = y_method[:noncommon_idx_method[0]] + [0] + y_method[1:] \n",
    "\n",
    "df_themes = pd.DataFrame({'Year':pd.Series(x_model), 'numpapers': pd.Series(numpapers),\n",
    "                          'models':pd.Series(y_model),\n",
    "                         'methods':pd.Series(new_y_method), 'concepts':pd.Series(y_concept)})\n",
    "\n",
    "#save df_themes including numpapers list \n",
    "df_themes.to_csv('/results/df_themes_numpapers.csv', index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8da07d-2507-40cf-b4f9-da5ec679ba85",
   "metadata": {},
   "source": [
    "## Create dataframe for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed59c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_models =  ['single server', 'multiserver', 'queueing network', 'polling', 'vacations', 'priority queue', 'tandem']\n",
    "lst_methods =  ['Laplace transforms','large deviations', 'diffusion', 'fluid limit']\n",
    "lst_concepts =  ['insensitivity', 'tail asymptotics', 'product form', 'reversibility',  'bandwidth', 'stability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9494f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of keywords with their proportion overtime and keeping lists same size\n",
    "x_ss , y_ss = count_theme_yrs(lst_models_yrs[0])\n",
    "x_ms , y_ms = count_theme_yrs(lst_models_yrs[1])\n",
    "x_qn , y_qn = count_theme_yrs(lst_models_yrs[2])\n",
    "x_pl , y_pl = count_theme_yrs(lst_models_yrs[3])\n",
    "x_vc , y_vc = count_theme_yrs(lst_models_yrs[4])\n",
    "x_pq , y_pq = count_theme_yrs(lst_models_yrs[5])\n",
    "x_tm , y_tm = count_theme_yrs(lst_models_yrs[6])\n",
    "\n",
    "x_lt , y_lt = count_theme_yrs(lst_methods_yrs[0])\n",
    "x_ld , y_ld = count_theme_yrs(lst_methods_yrs[1])\n",
    "x_df , y_df = count_theme_yrs(lst_methods_yrs[2])\n",
    "x_fl , y_fl = count_theme_yrs(lst_methods_yrs[3])\n",
    "\n",
    "x_in , y_in = count_theme_yrs(lst_concepts_yrs[0])\n",
    "x_ta , y_ta = count_theme_yrs(lst_concepts_yrs[1])\n",
    "x_pf , y_pf = count_theme_yrs(lst_concepts_yrs[2])\n",
    "x_rv , y_rv = count_theme_yrs(lst_concepts_yrs[3])\n",
    "x_bw , y_bw = count_theme_yrs(lst_concepts_yrs[4])\n",
    "x_st , y_st = count_theme_yrs(lst_concepts_yrs[5])\n",
    "\n",
    "year = x_ss   \n",
    "\n",
    "#define a function that creates a lst same size as list year\n",
    "def create_lst(l1, xlst, ylst):\n",
    "    xlst = set(xlst)\n",
    "    idx = [i for i, el in enumerate(l1) if el not in xlst]\n",
    "    l = ylst.copy()\n",
    "    newl = l\n",
    "    for  i in idx:\n",
    "        newl.insert(i,0)\n",
    "    return newl \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b45edccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new lsts for each theme\n",
    "newy_ss = create_lst(year, x_ss, y_ss)\n",
    "newy_ms = create_lst(year, x_ms, y_ms)\n",
    "newy_qn = create_lst(year, x_qn, y_qn)\n",
    "newy_pl = create_lst(year, x_pl, y_pl)\n",
    "newy_vc = create_lst(year, x_vc, y_vc)\n",
    "newy_pq = create_lst(year, x_pq, y_pq)\n",
    "newy_tm = create_lst(year, x_tm, y_tm)\n",
    "\n",
    "newy_lt = create_lst(year, x_lt, y_lt)\n",
    "newy_ld = create_lst(year, x_ld, y_ld)\n",
    "newy_df = create_lst(year, x_df, y_df)\n",
    "newy_fl = create_lst(year, x_fl, y_fl)\n",
    "\n",
    "newy_in = create_lst(year, x_in, y_in)\n",
    "newy_ta = create_lst(year, x_ta, y_ta)\n",
    "newy_pf = create_lst(year, x_pf, y_pf)\n",
    "newy_rv = create_lst(year, x_rv, y_rv)\n",
    "newy_bw = create_lst(year, x_bw, y_bw)\n",
    "newy_st = create_lst(year, x_st, y_st)\n",
    "\n",
    "#create a dictionary with keyword length\n",
    "prop_by_keywords = {\n",
    "    'single server': newy_ss,   \n",
    "     'multiserver': newy_ms,    \n",
    "     'queueing network': newy_qn,   \n",
    "     'polling': newy_pl,            \n",
    "     'vacations': newy_vc,       \n",
    "     'priority queue':newy_pq,     \n",
    "     'tandem':newy_tm,           \n",
    "     'Laplace transforms': newy_lt,\n",
    "     'large deviations': newy_ld,\n",
    "     'diffusion': newy_df,\n",
    "     'fluid limit':newy_fl,\n",
    "     'insensitivity':newy_in,\n",
    "     'tail asymptotics':newy_ta, \n",
    "      'product form': newy_pf,\n",
    "     'reversibility': newy_rv,\n",
    "     'bandwidth': newy_bw,\n",
    "     'stability': newy_st\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68c37a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crate a dataframe of keyword proportions with year\n",
    "df_keywords_yr = pd.DataFrame.from_dict(prop_by_keywords)\n",
    "\n",
    "df_keywords_yr = df_keywords_yr.T \n",
    "\n",
    "#rename all column names\n",
    "lst_colnames = year.copy()\n",
    "lst_colnames = [str(x) for x in lst_colnames] \n",
    "df_keywords_yr.columns = lst_colnames\n",
    "\n",
    "\n",
    "df_keywords_yr.index.name = 'keywords'\n",
    "df_keywords_yr\n",
    "\n",
    "#add themes column \n",
    "lst_keywords = [lst_models, lst_methods, lst_concepts]\n",
    "lst_keywords = [x for xs in lst_keywords for x in xs]  #unlist sublists\n",
    "type(lst_keywords)  #17keywords\n",
    "\n",
    "\n",
    "#create a list with theme name for each keyword\n",
    "lst_themes = []\n",
    "for l in lst_keywords:\n",
    "    if l in lst_models:\n",
    "        lst_themes.append(1)\n",
    "    elif l in lst_methods:\n",
    "         lst_themes.append(2)\n",
    "    else: \n",
    "         lst_themes.append(3)\n",
    "lst_themes\n",
    "\n",
    "df_keywords_yr['themes'] = lst_themes\n",
    "df_keywords_yr\n",
    "\n",
    "#save the dataframe\n",
    "df_keywords_yr.to_csv('/results/df_keywords_yr.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d063302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define new  periods\n",
    "period1 = [*range(1986, 1995, 1)]\n",
    "period2 = [*range(1995, 2004, 1)]\n",
    "period3 = [*range(2004, 2013, 1)]\n",
    "period4 = [*range(2013, 2022, 1)]\n",
    "\n",
    "#define list of periods and ranges\n",
    "periods = [period1, period2, period3, period4] # period5, period6, period7, period8, period9\n",
    "# period_ranges = ['1986 - 1989', '1990 - 1993', '1994 - 1997', '1998 - 2001', '2002 - 2005', '2006 - 2009', \n",
    "#                 '2010 - 2013', '2014 - 2017', '2018 - 2021']\n",
    "period_ranges = ['1986 - 1994',  '1995 - 2003', '2004 - 2012', '2013 - 2021']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a023264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all lemmatized_ themes into one list\n",
    "lemmatized_themes = [lemmatized_models, lemmatized_methods, lemmatized_concepts]\n",
    "lemmatized_themes = \" \".join(map(\" \" .join, lemmatized_themes)).split()\n",
    "# print(lemmatized_themes)\n",
    "\n",
    "#create dictionary for lemmatized_ themes and lst_ themes. Then merge all dictionaries\n",
    "dict_models = dict(zip(lemmatized_models, lst_models))\n",
    "dict_methods = dict(zip(lemmatized_methods, lst_methods))\n",
    "dict_concepts = dict(zip(lemmatized_concepts, lst_concepts))\n",
    "\n",
    "#merge dictionaries\n",
    "dict_themes = {**dict_models, **dict_methods, **dict_concepts}\n",
    "dict_themes;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "317b8e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Counter to count words in each period, and then filter out the counts for the themes\n",
    "\n",
    "def count_themes(themes, txt):\n",
    "    count = Counter(txt)\n",
    "    return {key: count[key] for key in count if key in themes}\n",
    "\n",
    "def count_period_papers(period):\n",
    "    #count_papers is a  collection Counter of the number of papers in each year\n",
    "    d = {key: count_papers[key] for key in count_papers if key in period}\n",
    "    total = sum(list(d.values())) #total numb. of papers in the given period\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f96c0",
   "metadata": {},
   "source": [
    "##  Create dataframe for keywords in 4 periods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6b3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all keywords\n",
    "lst_keywords = [lst_models, lst_methods, lst_concepts]\n",
    "lst_keywords = [x for xs in lst_keywords for x in xs]  #unlist sublists\n",
    "type(lst_keywords)  #17keywords\n",
    "\n",
    "\n",
    "#create a list with theeme name for each keyword\n",
    "lst_themes = []\n",
    "for l in lst_keywords:\n",
    "    if l in lst_models:\n",
    "        lst_themes.append(1)\n",
    "    elif l in lst_methods:\n",
    "         lst_themes.append(2)\n",
    "    else: \n",
    "         lst_themes.append(3)\n",
    "\n",
    "df_keywords = pd.DataFrame(lst_keywords, columns =['keywords'])\n",
    "df_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c02a000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the proportion of papers corresponding to each period\n",
    "for i in range(len(periods)):\n",
    "    period = periods[i]\n",
    "    txt_period = txt_period_tokens(period)\n",
    "    theme_counts = count_themes(lemmatized_themes, txt_period[1])\n",
    "    sorted_counts = sorted(theme_counts.items(),  key=lambda x: x[1]) #reverse =True,\n",
    "    x, y = map(list, zip(*sorted_counts))\n",
    "    total_papers = count_period_papers(period)  #total number of papers in the period\n",
    "    new_y = [round(s/total_papers,3) for s in y]  #proportion of counts per period\n",
    "    new_x = [dict_themes[w] for w in x] #{key: dict_themes[key] for key in dict_themes if key in x}\n",
    "\n",
    "    lst_p_values = []\n",
    "    for l in lst_keywords:\n",
    "        if l in new_x:\n",
    "        # lst_p1_values.append(l)\n",
    "            idx =   new_x.index(l)\n",
    "            lst_p_values.append(new_y[idx])\n",
    "        else:\n",
    "             lst_p_values.append(0)\n",
    "            \n",
    "    #j = 1+i\n",
    "    colname = str(period_ranges[i])\n",
    "    #colname = 'period' + str(j)\n",
    "    df_keywords[colname] = lst_p_values\n",
    "    #print(period_range)\n",
    "\n",
    "df_keywords['themes'] = lst_themes\n",
    "\n",
    "#save the dataframe\n",
    "df_keywords.to_csv(path + '/Results/df_keywords.csv', index =False)\n",
    "# df_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77ce9b",
   "metadata": {},
   "source": [
    "## Co-Occurance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b57e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of papers with co-occurance of words from the df_topics\n",
    "\n",
    "lst_ss_laptransform = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['laptransform']==True)] \n",
    "lst_ss_lrgdev = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['largedeviation']==True)] \n",
    "lst_ss_diff = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['diffusion']==True)] \n",
    "lst_ss_limitfluid  = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['limitfluid']==True)] \n",
    "                      # #concepts\n",
    "lst_ss_insensitivity  = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_ss_tailasymptotic = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['tailasymptotic']==True)]\n",
    "lst_ss_productform = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['productform']==True)]\n",
    "lst_ss_reversibility = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['reversibility']==True)]\n",
    "lst_ss_widthband = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['widthband']==True)]\n",
    "lst_ss_stability = df_topics.PubYear[(df_topics['singleserver']==True) & (df_topics['stability']==True)]\n",
    "# #model = 'multiserver'\n",
    "                 # #methods\n",
    "lst_ms_laptransform = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['laptransform']==True)] \n",
    "lst_ms_lrgdev = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['largedeviation']==True)] \n",
    "lst_ms_diff = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['diffusion']==True)] \n",
    "lst_ms_limitfluid  = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['limitfluid']==True)] \n",
    "                 # #concepts\n",
    "lst_ms_insensitivity  = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_ms_tailasymptotic = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['tailasymptotic']==True)]\n",
    "lst_ms_productform = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['productform']==True)]\n",
    "lst_ms_reversibility = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['reversibility']==True)]\n",
    "lst_ms_widthband = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['widthband']==True)]\n",
    "lst_ms_stability = df_topics.PubYear[(df_topics['multiserver']==True) & (df_topics['stability']==True)]\n",
    "#model = 'queuenetwork'\n",
    "                   #methods\n",
    "lst_qn_laptransform = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['laptransform']==True)] \n",
    "lst_qn_lrgdev = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['largedeviation']==True)] \n",
    "lst_qn_diff = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['diffusion']==True)] \n",
    "lst_qn_limitfluid  = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['limitfluid']==True)] \n",
    "                          # #concepts\n",
    "lst_qn_insensitivity  = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_qn_tailasymptotic = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['tailasymptotic']==True)]\n",
    "lst_qn_productform = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['productform']==True)]\n",
    "lst_qn_reversibility = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['reversibility']==True)]\n",
    "lst_qn_widthband = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['widthband']==True)]\n",
    "lst_qn_stability = df_topics.PubYear[(df_topics['queuenetwork']==True) & (df_topics['stability']==True)]\n",
    "\n",
    "# #model = 'poll'\n",
    "                   # #methods\n",
    "lst_pl_laptransform = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['laptransform']==True)] \n",
    "lst_pl_lrgdev = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['largedeviation']==True)] \n",
    "lst_pl_diff = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['diffusion']==True)] \n",
    "lst_pl_limitfluid  = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['limitfluid']==True)] \n",
    "                          # #concepts\n",
    "lst_pl_insensitivity  = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_pl_tailasymptotic = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['tailasymptotic']==True)]\n",
    "lst_pl_productform = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['productform']==True)]\n",
    "lst_pl_reversibility = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['reversibility']==True)]\n",
    "lst_pl_widthband = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['widthband']==True)]\n",
    "lst_pl_stability = df_topics.PubYear[(df_topics['poll']==True) & (df_topics['stability']==True)]\n",
    "\n",
    "# #model = 'vacation'\n",
    "                   # #methods\n",
    "lst_vn_laptransform = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['laptransform']==True)] \n",
    "lst_vn_lrgdev = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['largedeviation']==True)] \n",
    "lst_vn_diff = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['diffusion']==True)] \n",
    "lst_vn_limitfluid  = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['limitfluid']==True)] \n",
    "                          # #concepts\n",
    "lst_vn_insensitivity  = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_vn_tailasymptotic = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['tailasymptotic']==True)]\n",
    "lst_vn_productform = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['productform']==True)]\n",
    "lst_vn_reversibility = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['reversibility']==True)]\n",
    "lst_vn_widthband = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['widthband']==True)]\n",
    "lst_vn_stability = df_topics.PubYear[(df_topics['vacation']==True) & (df_topics['stability']==True)]\n",
    "\n",
    "# #model = 'priorityqueue'\n",
    "                   # #methods\n",
    "lst_pq_laptransform = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['laptransform']==True)] \n",
    "lst_pq_lrgdev = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['largedeviation']==True)] \n",
    "lst_pq_diff = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['diffusion']==True)] \n",
    "lst_pq_limitfluid  = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['limitfluid']==True)] \n",
    "                          # #concepts\n",
    "lst_pq_insensitivity  = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_pq_tailasymptotic = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['tailasymptotic']==True)]\n",
    "lst_pq_productform = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['productform']==True)]\n",
    "lst_pq_reversibility = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['reversibility']==True)]\n",
    "lst_pq_widthband = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['widthband']==True)]\n",
    "lst_pq_stability = df_topics.PubYear[(df_topics['priorityqueue']==True) & (df_topics['stability']==True)]\n",
    "\n",
    "\n",
    "# #model = 'tandem'\n",
    "                   # #methods\n",
    "lst_tm_laptransform = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['laptransform']==True)] \n",
    "lst_tm_lrgdev = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['largedeviation']==True)] \n",
    "lst_tm_diff = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['diffusion']==True)] \n",
    "lst_tm_limitfluid  = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['limitfluid']==True)] \n",
    "                          # #concepts\n",
    "lst_tm_insensitivity  = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_tm_tailasymptotic = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['tailasymptotic']==True)]\n",
    "lst_tm_productform = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['productform']==True)]\n",
    "lst_tm_reversibility = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['reversibility']==True)]\n",
    "lst_tm_widthband = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['widthband']==True)]\n",
    "lst_tm_stability = df_topics.PubYear[(df_topics['tandem']==True) & (df_topics['stability']==True)]\n",
    "\n",
    "################# method and concepts ###\n",
    "#method = 'laplace transform'\n",
    "lst_lt_in = df_topics.PubYear[(df_topics['laptransform']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_lt_ta = df_topics.PubYear[(df_topics['laptransform']==True) & (df_topics['tailasymptotic']==True)] \n",
    "lst_lt_pf = df_topics.PubYear[(df_topics['laptransform']==True) & (df_topics['productform']==True)]\n",
    "lst_lt_re = df_topics.PubYear[(df_topics['laptransform']==True) & (df_topics['reversibility']==True)] \n",
    "lst_lt_bw = df_topics.PubYear[(df_topics['laptransform']==True) & (df_topics['widthband']==True)] \n",
    "lst_lt_st = df_topics.PubYear[(df_topics['laptransform']==True) & (df_topics['stability']==True)]\n",
    "\n",
    "#method = 'large deviations'\n",
    "lst_ld_in = df_topics.PubYear[(df_topics['largedeviation']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_ld_ta = df_topics.PubYear[(df_topics['largedeviation']==True) & (df_topics['tailasymptotic']==True)] \n",
    "lst_ld_pf = df_topics.PubYear[(df_topics['largedeviation']==True) & (df_topics['productform']==True)]\n",
    "lst_ld_re = df_topics.PubYear[(df_topics['largedeviation']==True) & (df_topics['reversibility']==True)] \n",
    "lst_ld_bw = df_topics.PubYear[(df_topics['largedeviation']==True) & (df_topics['widthband']==True)] \n",
    "lst_ld_st = df_topics.PubYear[(df_topics['largedeviation']==True) & (df_topics['stability']==True)]\n",
    "\n",
    "#method = 'diffusion'\n",
    "lst_df_in = df_topics.PubYear[(df_topics['diffusion']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_df_ta = df_topics.PubYear[(df_topics['diffusion']==True) & (df_topics['tailasymptotic']==True)] \n",
    "lst_df_pf = df_topics.PubYear[(df_topics['diffusion']==True) & (df_topics['productform']==True)]\n",
    "lst_df_re = df_topics.PubYear[(df_topics['diffusion']==True) & (df_topics['reversibility']==True)] \n",
    "lst_df_bw = df_topics.PubYear[(df_topics['diffusion']==True) & (df_topics['widthband']==True)] \n",
    "lst_df_st = df_topics.PubYear[(df_topics['diffusion']==True) & (df_topics['stability']==True)]\n",
    "\n",
    "#method = 'fluid limit'\n",
    "lst_fl_in = df_topics.PubYear[(df_topics['limitfluid']==True) & (df_topics['insensitivity']==True)] \n",
    "lst_fl_ta = df_topics.PubYear[(df_topics['limitfluid']==True) & (df_topics['tailasymptotic']==True)] \n",
    "lst_fl_pf = df_topics.PubYear[(df_topics['limitfluid']==True) & (df_topics['productform']==True)]\n",
    "lst_fl_re = df_topics.PubYear[(df_topics['limitfluid']==True) & (df_topics['reversibility']==True)] \n",
    "lst_fl_bw = df_topics.PubYear[(df_topics['limitfluid']==True) & (df_topics['widthband']==True)] \n",
    "lst_fl_st = df_topics.PubYear[(df_topics['limitfluid']==True) & (df_topics['stability']==True)]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c75349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to count the number of papers in each co-occurance list for a given period\n",
    "\n",
    "def count_occurance(period, lst):\n",
    "    count_lst = Counter(lst)\n",
    "    d = {key: count_lst[key] for key in count_lst if key in period}\n",
    "    total = sum(list(d.values())) \n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ec5003f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For each period, count how many papers have the co-occurance words \n",
    "#period1\n",
    "occurance_lst = [lst_ss_laptransform, lst_ss_lrgdev, lst_ss_diff, lst_ss_limitfluid, \n",
    "                 lst_ss_insensitivity,lst_ss_tailasymptotic, lst_ss_productform , lst_ss_reversibility,\n",
    "                 lst_ss_widthband,lst_ss_stability,\n",
    "                 #\n",
    "                 lst_ms_laptransform, lst_ms_lrgdev, lst_ms_diff, lst_ms_limitfluid, \n",
    "                 lst_ms_insensitivity,lst_ms_tailasymptotic, lst_ms_productform , lst_ms_reversibility,\n",
    "                 lst_ms_widthband,lst_ms_stability,\n",
    "                #\n",
    "                 lst_qn_laptransform, lst_qn_lrgdev, lst_qn_diff, lst_qn_limitfluid, \n",
    "                 lst_qn_insensitivity,lst_qn_tailasymptotic, lst_qn_productform , lst_qn_reversibility,\n",
    "                 lst_qn_widthband,lst_qn_stability,\n",
    "                #\n",
    "                 lst_pl_laptransform, lst_pl_lrgdev, lst_pl_diff, lst_pl_limitfluid, \n",
    "                 lst_pl_insensitivity,lst_pl_tailasymptotic, lst_pl_productform , lst_pl_reversibility,\n",
    "                 lst_pl_widthband,lst_pl_stability,\n",
    "                #\n",
    "                 lst_vn_laptransform, lst_vn_lrgdev, lst_vn_diff, lst_vn_limitfluid, \n",
    "                 lst_vn_insensitivity,lst_vn_tailasymptotic, lst_vn_productform , lst_vn_reversibility,\n",
    "                 lst_vn_widthband,lst_vn_stability,\n",
    "                 #\n",
    "                 lst_pq_laptransform, lst_pq_lrgdev, lst_pq_diff, lst_pq_limitfluid, \n",
    "                 lst_pq_insensitivity,lst_pq_tailasymptotic, lst_pq_productform , lst_pq_reversibility,\n",
    "      \n",
    "                 lst_pq_widthband,lst_pq_stability,\n",
    "                #\n",
    "                 lst_tm_laptransform, lst_tm_lrgdev, lst_tm_diff, lst_tm_limitfluid, \n",
    "                 lst_tm_insensitivity,lst_tm_tailasymptotic, lst_tm_productform , lst_tm_reversibility,\n",
    "                 lst_tm_widthband,lst_tm_stability,\n",
    "                #\n",
    "                lst_lt_in, lst_lt_ta, lst_lt_pf, lst_lt_re, lst_lt_bw, lst_lt_st,\n",
    "                #\n",
    "                lst_ld_in, lst_ld_ta, lst_ld_pf, lst_ld_re, lst_ld_bw, lst_ld_st,\n",
    "                #\n",
    "                 lst_df_in, lst_df_ta, lst_df_pf, lst_df_re, lst_df_bw, lst_df_st,\n",
    "                 #\n",
    "                 lst_fl_in, lst_fl_ta, lst_fl_pf, lst_fl_re, lst_fl_bw, lst_fl_st\n",
    "                ]\n",
    "#df_occurance_period1 = Dataframe()\n",
    "len(occurance_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b0a77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurance_period(period):\n",
    "    period = period\n",
    "    lst=[]\n",
    "    for occurance in occurance_lst:\n",
    "    #print(lst)\n",
    "        total = count_occurance(period, occurance)\n",
    "        lst.append(total)\n",
    "    return lst\n",
    "\n",
    "    #txt_period = txt_period_tokens(period)\n",
    "#print(lst_count_occurance_period1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27ac94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the count of co-occurance of words in each period\n",
    "lst_count_occurance_period1 = count_occurance_period(period1)\n",
    "lst_count_occurance_period2 = count_occurance_period(period2)\n",
    "lst_count_occurance_period3 = count_occurance_period(period3)\n",
    "lst_count_occurance_period4 = count_occurance_period(period4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "837c320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe for each period\n",
    "\n",
    "#for this, first create tupule of words\n",
    "\n",
    "names_lst = [('single server', 'Laplace transforms'), ('single server', 'large deviations'),\n",
    "          ('single server', 'diffusion'), ('single server', 'fluid limit'),\n",
    "         ('single server', 'insensitivity'), ('single server', 'tail asymptotics'), \n",
    "          ('single server', 'product form'), ('single server', 'reversibility'), ('single server', 'bandwidth'),\n",
    "         ('single server', 'stability'),\n",
    "          #\n",
    "         ('multiserver', 'Laplace transforms'), ('multiserver', 'large deviations'),\n",
    "          ('multiserver', 'diffusion'), ('multiserver', 'fluid limit'),\n",
    "         ('multiserver', 'insensitivity'), ('multiserver', 'tail asymptotics'), \n",
    "          ('multiserver', 'product form'), ('multiserver', 'reversibility'), ('multiserver', 'bandwidth'),\n",
    "         ('multiserver', 'stability'),\n",
    "          #\n",
    "         ('queueing network', 'Laplace transforms'), ('queueing network', 'large deviations'),\n",
    "          ('queueing network', 'diffusion'), ('queueing network', 'fluid limit'),\n",
    "         ('queueing network', 'insensitivity'), ('queueing network', 'tail asymptotics'), \n",
    "          ('queueing network', 'product form'), ('queueing network', 'reversibility'), ('queueing network', 'bandwidth'),\n",
    "         ('queueing network', 'stability'),\n",
    "          #\n",
    "         ('polling', 'Laplace transforms'), ('polling', 'large deviations'),\n",
    "          ('polling', 'diffusion'), ('polling', 'fluid limit'),\n",
    "         ('polling', 'insensitivity'), ('polling', 'tail asymptotics'), \n",
    "          ('polling', 'product form'), ('polling', 'reversibility'), ('polling', 'bandwidth'),\n",
    "         ('polling', 'stability'),\n",
    "         #\n",
    "         ('vacations', 'Laplace transforms'), ('vacations', 'large deviations'),\n",
    "          ('vacations', 'diffusion'), ('vacations', 'fluid limit'),\n",
    "         ('vacations', 'insensitivity'), ('vacations', 'tail asymptotics'), \n",
    "          ('vacations', 'product form'), ('vacations', 'reversibility'), ('vacations', 'bandwidth'),\n",
    "         ('vacations', 'stability'),\n",
    "         #\n",
    "         ('priority queue', 'Laplace transforms'), ('priority queue', 'large deviations'),\n",
    "          ('priority queue', 'diffusion'), ('priority queue', 'fluid limit'),\n",
    "         ('priority queue', 'insensitivity'), ('priority queue', 'tail asymptotics'), \n",
    "          ('priority queue', 'product form'), ('priority queue', 'reversibility'), ('priority queue', 'bandwidth'),\n",
    "         ('priority queue', 'stability'),\n",
    "         #\n",
    "         ('tandem', 'Laplace transforms'), ('tandem', 'large deviations'),\n",
    "          ('tandem', 'diffusion'), ('tandem', 'fluid limit'),\n",
    "         ('tandem', 'insensitivity'), ('tandem', 'tail asymptotics'), \n",
    "          ('tandem', 'product form'), ('tandem', 'reversibility'), ('tandem', 'bandwidth'),\n",
    "         ('tandem', 'stability'),\n",
    "             #\n",
    "            ('Laplace transforms', 'insensitivity'), ('Laplace transforms', 'tail asymptotics'),\n",
    "             ('Laplace transforms', 'product form'), ('Laplace transforms', 'reversibility'), \n",
    "             ('Laplace transforms', 'bandwidth'), ('Laplace transforms', 'stability'),\n",
    "            #\n",
    "             ('Large deviations', 'insensitivity'), ('Large deviations', 'tail asymptotics'),\n",
    "             ('Large deviations', 'product form'), ('Large deviations', 'reversibility'), \n",
    "             ('Large deviations', 'bandwidth'), ('Large deviations', 'stability'),\n",
    "             #\n",
    "             ('diffusion', 'insensitivity'), ('diffusion', 'tail asymptotics'),\n",
    "             ('diffusion', 'product form'), ('diffusion', 'reversibility'), \n",
    "             ('diffusion', 'bandwidth'), ('diffusion', 'stability'),\n",
    "             #\n",
    "             ('fluid limit', 'insensitivity'), ('fluid limit', 'tail asymptotics'),\n",
    "             ('fluid limit', 'product form'), ('fluid limit', 'reversibility'), \n",
    "             ('fluid limit', 'bandwidth'), ('fluid limit', 'stability')]\n",
    "#print(lst_models)\n",
    "print(len(names_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3c7c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occurance = pd.DataFrame(list(zip(names_lst, lst_count_occurance_period1,lst_count_occurance_period2,\n",
    "                                    lst_count_occurance_period3, lst_count_occurance_period4)#, \n",
    "                                    #lst_count_occurance_period5, lst_count_occurance_period6, \n",
    "                                    #lst_count_occurance_period7, lst_count_occurance_period8,\n",
    "                                    #lst_count_occurance_period9)\n",
    "                                ),\n",
    "               columns =['bigram', 'period1', 'period2', 'period3', 'period4'])#, 'period5','period6',\n",
    "                        #'period7', 'period8', 'period9'])\n",
    "#save the df_occurance to csv file\n",
    "df_occurance.to_csv('/results/update_abstract_co_occurance.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 (questa_venv)",
   "language": "python",
   "name": "questa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
